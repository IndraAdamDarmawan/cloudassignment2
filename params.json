{"name":"Indra's IS429 Assignment 2","tagline":"Big Data Processing Tutorial","body":"### Welcome to Cloud & Big Data Tutorial.\r\nThis is a tutorial for processing large datasets that can only be processed with high computational power that is cloud. In this tutorial we are going to use Amazon Elastic MapReducer (EMR), an amazingly powerful data processing service provided by Amazon Web Service. By following this tutorial, you can harness the power of cloud for processing big data in a matter of minutes.\r\nThis tutorial is divided into three parts, namely:\r\n\r\n```\r\n- Configuring EC2 and Security Credentials\r\n- Setting up S3\r\n- Running EMR\r\n```\r\n\r\nTo run this tutorial, you will need to sign up for [AWS account](http://aws.amazon.com). \r\n\r\n### Configuring EC2 and Security Credentials\r\nTo be able to run the Elastic MapReducer smoothly, we will need to make sure that EC2 and Security Credentials are configured correctly.\r\n\r\nCreate EC2 Key Pair:  \r\n* Go to your [AWS Management Console](https://console.aws.amazon.com/console/home)\r\n* Click on EC2 icon\r\n* Click on Key Pair\r\n* Click on Create Key Pair and give a unique name for it\r\n* Save the private key to your computer\r\n\r\nConfigure Security Credentials\r\n* Click on Security Credentials tab under you account name on the top right\r\n* Click on the Access Key tab under Access Credentials\r\n* Make sure there is an access key inside or create new one if you don't have any\r\n\r\n### Setting up Simple Storage Service (S3)\r\nAfter we configured the EC2, now we are going to play with the data. For this tutorial, we are going to use the sample datasets that can downloaded from the following link:\r\nAmazon S3:\r\nDropbox: \r\n\r\nThe zip file that you download will contain two files, sample-data.txt and wordSplitter.py. Unzip the zip file and follow these steps to upload it into the Amazon Simple Storage Service (S3)\r\nSetting up S3:\r\n* Go to your [AWS Management Console](https://console.aws.amazon.com/console/home)\r\n* Click on S3 icon\r\n* Create new Bucket and give a unique name to it\r\n* Choose Singapore as the Region\r\n* Upload the downloaded wordSplitter.py\r\n* Create new folder and named it 'input'\r\n* Upload the sample-data.txt into the 'input' folder\r\n\r\n### Running Elastic MapReduce (EMR)\r\nThe last step of our tutorial, in this step we will be processing the data that we uploaded into Amazon S3 using Amazon Elastic MapReduce. Follow these steps to run the EMR:\r\n* Go to your [AWS Management Console](https://console.aws.amazon.com/console/home)\r\n* Click on Elastic MapReduce icon\r\n* Click on Create New Job Flow\r\n* Give any name it doesn't have to be unique\r\n* Click on the dropdown list and choose 'Streaming', click continue\r\n* Insert your S3 bucket name/input (e.g. 'myawesomebucket/input') for the Input Location\r\n* Insert your S3 bucket name/output (e.g. 'myawesomebucket/output') for the Output Location\r\n* Insert your S3 bucket name/wordSplitter.py (e.g. 'myawesomebucket/wordSplitter.py') for the Mapper\r\n* Insert 'aggregate' for the Reducer, click continue\r\n* Don't change anything in 'Configure EC2 Instances', click continue\r\n* Click on the Amazon EC2 Key Pair's dropdown list and choose the key pair that we have just created at the precious step, click continue\r\n* Don't change anything in 'Bootstrap Actions', click continue\r\n* Don't change anything in 'Review', click Create Job Flow\r\n* Click close\r\n\r\nNow you can see that the Job Flow is running, you can see the progress by clicking on it. You can go grab a cup of coffee while the cloud is processing the big data.\r\n\r\n### Geting the Results\r\nWhen the Job Flow is completed, you can see the result by going to the S3 bucket that we created earlier. Click on the output folder and download 'part-000', which contains the results.\r\n\r\nCongratulations you have just become more awesome.\r\n\r\n### Support or Contact\r\nIf you encounter any problem while following this tutorial, you can email me at indraadam.darmawan@gmail.com. Alternatively you can check [AWS Forum](forums.aws.amazon.com) for any enquiry or information regarding Amazon Web Service.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}