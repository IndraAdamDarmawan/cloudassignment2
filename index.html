<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8' />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="description" content="Indra's IS429 Assignment 2 : Big Data Processing Tutorial" />

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Indra's IS429 Assignment 2</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/IndraAdamDarmawan/cloudassignment2">View on GitHub</a>

          <h1 id="project_title">Indra's IS429 Assignment 2</h1>
          <h2 id="project_tagline">Big Data Processing Tutorial</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/IndraAdamDarmawan/cloudassignment2/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/IndraAdamDarmawan/cloudassignment2/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h3>Welcome to Cloud &amp; Big Data Tutorial.</h3>

<p>This is a tutorial for processing large datasets that can only be processed with high computational power that is cloud. In this tutorial we are going to use Amazon Elastic MapReducer (EMR), an amazingly powerful data processing service provided by Amazon Web Service. By following this tutorial, you can harness the power of cloud for processing big data in a matter of minutes.
This tutorial is divided into three parts, namely:</p>

<pre><code>- Configuring EC2 and Security Credentials
- Setting up S3
- Running EMR
</code></pre>

<p>To run this tutorial, you will need to sign up for <a href="http://aws.amazon.com">AWS account</a>. </p>

<h3>Configuring EC2 and Security Credentials</h3>

<p>To be able to run the Elastic MapReducer smoothly, we will need to make sure that EC2 and Security Credentials are configured correctly.</p>

<p>Create EC2 Key Pair:  </p>

<pre><code>- Go to your [AWS Management Console](https://console.aws.amazon.com/console/home)
- Click on EC2 icon
- Click on Key Pair
- Click on Create Key Pair and give a unique name for it
- Save the private key to your computer
</code></pre>

<p>Configure Security Credentials:</p>

<pre><code>- Click on Security Credentials tab under you account name on the top right
- Click on the Access Key tab under Access Credentials
- Make sure there is an access key inside or create new one if you don't have any
</code></pre>

<h3>Setting up Simple Storage Service (S3)</h3>

<p>After we configured the EC2, now we are going to play with the data. For this tutorial, we are going to use the sample datasets that can downloaded from the following link:</p>

<ul>
<li>Amazon S3:</li>
<li>Dropbox: </li>
</ul><p>The zip file that you download will contain two files, sample-data.txt and wordSplitter.py. Unzip the zip file and follow these steps to upload it into the Amazon Simple Storage Service (S3)
Setting up S3:</p>

<pre><code>- Go to your [AWS Management Console](https://console.aws.amazon.com/console/home)
- Click on S3 icon
- Create new Bucket and give a unique name to it
- Choose Singapore as the Region
- Upload the downloaded wordSplitter.py
- Create new folder and named it 'input'
- Upload the sample-data.txt into the 'input' folder
</code></pre>

<h3>Running Elastic MapReduce (EMR)</h3>

<p>The last step of our tutorial, in this step we will be processing the data that we uploaded into Amazon S3 using Amazon Elastic MapReduce. Follow these steps to run the EMR:</p>

<pre><code>- Go to your [AWS Management Console](https://console.aws.amazon.com/console/home)
- Click on Elastic MapReduce icon
- Click on Create New Job Flow
- Give any name it doesn't have to be unique
- Click on the dropdown list and choose 'Streaming', click continue
- Insert your S3 bucket name/input (e.g. 'myawesomebucket/input') 
  for the Input Location
- Insert your S3 bucket name/output (e.g. 'myawesomebucket/output') 
  for the Output Location
- Insert your S3 bucket name/wordSplitter.py (e.g. 'myawesomebucket/wordSplitter.py') 
  for the Mapper
- Insert 'aggregate' for the Reducer, click continue
- Don't change anything in 'Configure EC2 Instances', click continue
- Click on the Amazon EC2 Key Pair's dropdown list and choose the key pair 
  that we have just created at the precious step, click continue
- Don't change anything in 'Bootstrap Actions', click continue
- Don't change anything in 'Review', click Create Job Flow
- Click close
</code></pre>

<p>Now you can see that the Job Flow is running, you can see the progress by clicking on it. You can go grab a cup of coffee while the cloud is processing the big data.</p>

<h3>Getting the Results</h3>

<p>When the Job Flow is completed, you can see the result by going to the S3 bucket that we created earlier. Click on the output folder and download 'part-000', which contains the results.</p>

<p>Congratulations you have just become more awesome.</p>

<h3>Support or Contact</h3>

<p>If you encounter any problem while following this tutorial, you can email me at indraadam.darmawan@gmail.com. Alternatively you can check <a href="forums.aws.amazon.com">AWS Forum</a> for any enquiry or information regarding Amazon Web Service.</p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Indra's IS429 Assignment 2 maintained by <a href="https://github.com/IndraAdamDarmawan">IndraAdamDarmawan</a></p>
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
